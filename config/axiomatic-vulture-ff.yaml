base:
  feedback_prize_effectiveness_dir: input/feedback-prize-effectiveness/
  feedback_prize_2021_dir: input/feedback-prize-2021/
  input_data_dir: input/feedback-prize-effectiveness-st
  output_dir: output
architecture:
  add_wide_dropout: false
  aux_type: false
  backbone: microsoft/deberta-v3-large
  custom_intermediate_dropout: false
  dropout: 0
  intermediate_dropout: 0.1
  gradient_checkpointing: false
  model_class: feedback_essay_model
  pool: All [CLS] token
  pretrained_weights: ""
  use_type: false
dataset:
  add_group_types: true
  dataset_class: feedback_dataset_essay_ds
  group_discourse: true
  fold: 0
  label_columns: 
  - discourse_effectiveness_Adequate
  - discourse_effectiveness_Effective
  - discourse_effectiveness_Ineffective
  num_classes: 3
  separator: ""
  text_column: 
  - essay_text
  train_df_path: input/feedback-prize-effectiveness-st/train_folded.csv
environment:
  mixed_precision: true
  num_workers: 4
  report_to: []
  seed: 1
  debug: false
tokenizer:
  add_newline_token: true
  max_length: 2048
training:
  aux_loss_function: CrossEntropy
  add_types: false
  batch_size: 1
  drop_last_batch: true
  epochs: 200
  grad_accumulation: 9
  gradient_clip: 0
  is_pseudo: false
  learning_rate: 3.0e-05
  differential_learning_rate: 1.0e-05
  differential_learning_rate_layers: []
  loss_function: CrossEntropy
  optimizer: AdamW
  schedule: Linear
  warmup_epochs: 0
  weight_decay: 0