architecture:
  add_wide_dropout: false
  backbone: microsoft/deberta-large
  dropout: 0.1
  gradient_checkpointing: true
  pretrained_weights: "output/2022-09-17-152432/checkpoint.pth"
dataset:
  dataset_class: feedback_dataset
  fold: 0
  label_columns: tokens
  num_classes: 3
  text_column: essay_text
  train_df_path: input/feedback-prize-effectiveness-st/feedback_text_token_classification_v5.pq
environment:
  mixed_precision: true
  num_workers: 4
  report_to: []
  seed: 1
  debug: true
tokenizer:
  max_length: 512
training:
  add_types: false
  batch_size: 2
  differential_learning_rate: 1.0e-05
  differential_learning_rate_layers: []
  drop_last_batch: true
  epochs: 3
  grad_accumulation: 1
  gradient_clip: 5
  is_pseudo: false
  learning_rate: 2.0e-05
  loss_function: CrossEntropy
  optimizer: AdamW
  schedule: Linear
  warmup_epochs: 1.5
  weight_decay: 0.001
