base:
  feedback_prize_effectiveness_dir: input/feedback-prize-effectiveness/
  feedback_prize_2021_dir: input/feedback-prize-2021/
  input_data_dir: input/feedback-prize-effectiveness-st
  output_dir: output
architecture:
  add_wide_dropout: false
  backbone: microsoft/deberta-large
  dropout: 0.1
  gradient_checkpointing: true
  pretrained_weights: ""
dataset:
  dataset_class: feedback_dataset
  fold: 0
  label_columns: tokens
  num_classes: 3
  text_column: essay_text
  train_df_path: input/feedback-prize-effectiveness-st/feedback_text_token_classification_v5.pq
environment:
  mixed_precision: true
  num_workers: 4
  report_to: []
  seed: -1
  debug: true
tokenizer:
  max_length: 2048
training:
  add_types: false
  batch_size: 1
  differential_learning_rate: 1.0e-05
  differential_learning_rate_layers: []
  drop_last_batch: true
  epochs: 2
  grad_accumulation: 1
  gradient_clip: 1
  is_pseudo: false
  learning_rate: 3.0e-05
  loss_function: CrossEntropy
  optimizer: AdamW
  schedule: Linear
  warmup_epochs: 1
  weight_decay: 0.01
